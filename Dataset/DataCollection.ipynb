{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install arxiv==1.4.2\n",
    "!pip install datetime==4.4\n",
    "!pip install PyMuPDF==1.19.6\n",
    "!pip install syllables==1.0.3\n",
    "!pip install matplotlib==3.5.2\n",
    "!pip install textstat==0.7.3\n",
    "!pip install statsmodels==0.13.2\n",
    "!pip install tensorflow-hub==0.12.0\n",
    "!pip install scipy==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import arxiv\n",
    "import datetime\n",
    "import fitz\n",
    "import re\n",
    "import syllables\n",
    "import random\n",
    "import unicodedata\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import textstat\n",
    "from statsmodels.api import OLS\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "import tensorflow_hub as hub\n",
    "from math import dist\n",
    "import copy\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_loc = os.path.dirname(os.path.dirname(os.path.abspath(\"DataCollection.py\")))\n",
    "suitable = []\n",
    "with open(parent_loc+'\\\\suitable.txt') as file:\n",
    "    suitable = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yetToBeDownloaded = suitable.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yetToBeDownloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pre in suitable:\n",
    "    yetToBeDownloaded.append(pre[:-1]+\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(yetToBeDownloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = {}\n",
    "all_keys = []\n",
    "for i in range(len(suitable)):\n",
    "    all_links[suitable[i][-12:-2]] = [suitable[0][:-1]+'1',suitable[i]]\n",
    "    all_keys.append(suitable[i][-12:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile(\"timings.npy\")):\n",
    "    # Load\n",
    "    all_timings = np.load('timings.npy',allow_pickle='TRUE').item()\n",
    "else:\n",
    "    all_timings = {}\n",
    "    for k in range(1000):\n",
    "        if (k%50==0):\n",
    "            print(f\"{k//10}%\")\n",
    "        paper = next(arxiv.Search(id_list=[yetToBeDownloaded[k][-12:]]).results())\n",
    "        all_timings[yetToBeDownloaded[k][-12:-2]] = (paper.updated-paper.published).total_seconds()\n",
    "\n",
    "    # Save\n",
    "    np.save('timings.npy', all_timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadPDF(index):\n",
    "    paper = next(arxiv.Search(id_list=[yetToBeDownloaded[index][-12:]]).results())\n",
    "    # Download the PDF to a specified directory with a custom filename.\n",
    "    if (index < 1000):\n",
    "        paper.download_pdf(filename=all_keys[index]+\".pdf\")\n",
    "    else:\n",
    "        paper.download_pdf(filename=all_keys[index%1000]+\"-Preprint.pdf\")\n",
    "\n",
    "def findAbstract(document):\n",
    "    for i in range(len(document)):\n",
    "        if str(doc[i]).lower() == \"abstract\":\n",
    "            return i\n",
    "    return -777\n",
    "\n",
    "def reverseSearchVertical(document):\n",
    "    for k in range(j-1, -1, -1):\n",
    "        if len(str(doc[k]).strip()) > 1:\n",
    "            return k\n",
    "        \n",
    "def GetAbstract(bigText):\n",
    "    for m in range(len(bigText.splitlines())):\n",
    "        if bigText.splitlines()[m].lower().replace(\" \",\"\") == 'abstract':\n",
    "            return m\n",
    "    return -1\n",
    "\n",
    "def GetReferences(bigText):\n",
    "    for n in range(len(bigText.splitlines())-1, -1, -1):\n",
    "        if bigText.splitlines()[n].lower().replace(\" \",\"\") == 'references':\n",
    "            return n\n",
    "    return -1\n",
    "\n",
    "def CropText(bigText):\n",
    "    begin = GetAbstract(bigText)\n",
    "    end = GetReferences(bigText)\n",
    "\n",
    "    if begin == -1 and end == -1:\n",
    "        return bigText.splitlines()[:]\n",
    "    elif begin == -1:\n",
    "        return bigText.splitlines()[:end]\n",
    "    elif end == -1:\n",
    "        return bigText.splitlines()[begin+1:]\n",
    "    else:\n",
    "        return bigText.splitlines()[begin+1:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessing(index):\n",
    "    \n",
    "    with fitz.open(f\"{working_keys[index]}-Preprint.pdf\") as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    \n",
    "    # Remove abstract and references\n",
    "    text = CropText(text)\n",
    "    \n",
    "    # Replace ligatures with single characters\n",
    "    testing = [unicodedata.normalize(\"NFKD\",part) for part in text]\n",
    "    \n",
    "    # Concatenate parts with - splitting them\n",
    "    fixedSentences = []\n",
    "    brokenSentence = \"\"\n",
    "    state = \"default\"\n",
    "    for sentence in testing:\n",
    "        if state == \"default\":\n",
    "            if \"arxi\" in sentence.lower():\n",
    "                pass\n",
    "            elif sentence[-1:] == \"-\":\n",
    "                state = \"broken\"\n",
    "                brokenSentence = sentence[:-1]\n",
    "            else:\n",
    "                fixedSentences.append(sentence)\n",
    "        elif state == \"broken\":\n",
    "            if \"arxi\" in sentence.lower():\n",
    "                pass\n",
    "            elif sentence[-1:] == \"-\":\n",
    "                brokenSentence = brokenSentence + sentence[:-1]\n",
    "            else:\n",
    "                state = \"default\"\n",
    "                brokenSentence = brokenSentence + sentence\n",
    "                fixedSentences.append(brokenSentence)\n",
    "                brokenSentence = \"\"\n",
    "                \n",
    "    # Remove links\n",
    "    testing = []\n",
    "    for sentence in fixedSentences:\n",
    "        if 'www' in sentence or 'http' in sentence:\n",
    "            testing.append(' '.join([word for word in sentence.split() if 'www' not in word and 'http' not in word]))\n",
    "        else:\n",
    "            testing.append(sentence)\n",
    "            \n",
    "    # Remove numbers in the text\n",
    "    noNumbers = []\n",
    "    for s in testing:\n",
    "        noNumbers.append(re.sub(r\"\\d+\", \"\", s))\n",
    "        \n",
    "    # Transform to lowercase only\n",
    "    lowerText = [sentence.lower() for sentence in noNumbers]\n",
    "    \n",
    "    # Only keep A-Z . and -\n",
    "    cleanText = [re.sub(\"[^ .!?a-zA-Z]+\", '', part) for part in lowerText]\n",
    "    cleanText = [re.sub(\"[!?]+\", '.', part) for part in cleanText]\n",
    "    \n",
    "    #========================================================================================\n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    with fitz.open(f\"{working_keys[index]}.pdf\") as doc:\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    \n",
    "    # Remove abstract and references\n",
    "    text = CropText(text)\n",
    "    \n",
    "    # Replace ligatures with single characters\n",
    "    testing = [unicodedata.normalize(\"NFKD\",part) for part in text]\n",
    "    \n",
    "    # Concatenate parts with - splitting them\n",
    "    fixedSentences = []\n",
    "    brokenSentence = \"\"\n",
    "    state = \"default\"\n",
    "    for sentence in testing:\n",
    "        if state == \"default\":\n",
    "            if \"arxi\" in sentence.lower():\n",
    "                pass\n",
    "            elif sentence[-1:] == \"-\":\n",
    "                state = \"broken\"\n",
    "                brokenSentence = sentence[:-1]\n",
    "            else:\n",
    "                fixedSentences.append(sentence)\n",
    "        elif state == \"broken\":\n",
    "            if \"arxi\" in sentence.lower():\n",
    "                pass\n",
    "            elif sentence[-1:] == \"-\":\n",
    "                brokenSentence = brokenSentence + sentence[:-1]\n",
    "            else:\n",
    "                state = \"default\"\n",
    "                brokenSentence = brokenSentence + sentence\n",
    "                fixedSentences.append(brokenSentence)\n",
    "                brokenSentence = \"\"\n",
    "                \n",
    "    # Remove links\n",
    "    testing = []\n",
    "    for sentence in fixedSentences:\n",
    "        if 'www' in sentence or 'http' in sentence:\n",
    "            testing.append(' '.join([word for word in sentence.split() if 'www' not in word and 'http' not in word]))\n",
    "        else:\n",
    "            testing.append(sentence)\n",
    "            \n",
    "    # Remove numbers in the text\n",
    "    noNumbers = []\n",
    "    for s in testing:\n",
    "        noNumbers.append(re.sub(r\"\\d+\", \"\", s))\n",
    "        \n",
    "    # Transform to lowercase only\n",
    "    lowerText = [sentence.lower() for sentence in noNumbers]\n",
    "    \n",
    "    # Only keep A-Z . and -\n",
    "    cleanText2 = [re.sub(\"[^ .!?a-zA-Z]+\", '', part) for part in lowerText]\n",
    "    cleanText2 = [re.sub(\"[!?]+\", '.', part) for part in cleanText2]\n",
    "    \n",
    "    return [cleanText, cleanText2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSentences(inputText):\n",
    "    return len([x for x in ' '.join(inputText).split('.') if len(x.replace(' ','')) > 1])\n",
    "\n",
    "def GetCleanWords(inputText):\n",
    "    return [y for y in ' '.join(inputText).replace('.','').split(' ') if y.replace(' ','') != '']\n",
    "\n",
    "def GetWords(cleanWords):\n",
    "    return len(cleanWords)\n",
    "\n",
    "def GetCharacters(cleanWords):\n",
    "    return sum([len(z) for z in cleanWords])\n",
    "\n",
    "def GetStatistics(inputtext):\n",
    "    sentences = GetSentences(inputtext)\n",
    "    cleanwords = GetCleanWords(inputtext)\n",
    "    words = GetWords(cleanwords)\n",
    "    characters = GetCharacters(cleanwords)\n",
    "    return sentences, words, characters, cleanwords\n",
    "\n",
    "def GetARI(number_of_sentences, number_of_words, number_of_characters):\n",
    "    c_w = number_of_characters / number_of_words\n",
    "    w_s = number_of_words / number_of_sentences\n",
    "    return (4.71 * c_w + 0.5 * w_s - 21.43)\n",
    "\n",
    "def GetFRES(number_of_sentences, number_of_words, number_of_clean_words):\n",
    "    se = sum([syllables.estimate(w) for w in number_of_clean_words])\n",
    "    s_w = se / number_of_words\n",
    "    w_s = number_of_words / number_of_sentences\n",
    "    return (206.835 - (1.015 * w_s) - (84.6 * s_w))\n",
    "    \n",
    "def GetFORCAST(cleanWords):\n",
    "    random.seed(777)\n",
    "    list_syllables = random.sample([syllables.estimate(w) for w in cleanWords], 150)\n",
    "    number_of_single_syllable_words = len([v for v in list_syllables if v == 1])\n",
    "    return (20 - (number_of_single_syllable_words / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Download(start, end):\n",
    "    for j in range(start, end):\n",
    "        if (j % 50 == 0):\n",
    "            print(f\"{j*100/2000}%\")\n",
    "        try:\n",
    "            DownloadPDF(j)\n",
    "            print(f\"Succes {j}\")\n",
    "        except:\n",
    "            print(f\"Failed to download for number {j} in the yetToBeDownloaded list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMeanVectorUSE(listText):\n",
    "    return np.mean(embed(listText), axis=0)\n",
    "\n",
    "def CalculateDistanceUSE(inputText1, inputText2):\n",
    "    if (inputText1 == []) or (inputText2 == []):\n",
    "        return -1\n",
    "    return dist(GetMeanVectorUSE(inputText1), GetMeanVectorUSE(inputText2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to False if you don't have the dataset downloaded yet\n",
    "alreadyDownloaded = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(0,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(500,750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(750,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(1000,1250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(1250,1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(1500,1750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not alreadyDownloaded:\n",
    "    Download(1750,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the usable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unusable = {2205.07192, 2203.10441, 2201.12276, 2203.07551, 2202.09965, 2202.03537, 2202.03161, 2111.14712, 2203.06123, 2203.07070, 2112.15408, 2204.01142, 2201.08582, 2112.14102, 2112.06780, 2112.05964}\n",
    "unusableIDs = [str(z) for z in unusable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_idx = [all([y not in x for y in unusableIDs]) for x in all_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_keys = list(np.array(all_keys)[np.array(bool_idx, dtype=bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(working_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "working_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile(\"filenames.npy\")):\n",
    "    # Load\n",
    "    all_files = np.load('filenames.npy',allow_pickle='TRUE').item()\n",
    "else:\n",
    "    all_files = {}\n",
    "    for idx in range(len(working_keys)):\n",
    "        all_files[idx] =[f\"{working_keys[idx]}.pdf\",f\"{working_keys[idx]}-Preprint.pdf\"]\n",
    "    np.save('filenames.npy', all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile(\"cleanText.npy\")):\n",
    "    # Load\n",
    "    all_text = np.load('cleanText.npy',allow_pickle='TRUE').item()\n",
    "else:\n",
    "    all_text = {}\n",
    "    for idx in range(len(working_keys)):\n",
    "        if (idx%50==0):\n",
    "            print(f\"{idx//10}%\")\n",
    "        all_text[idx] = PreProcessing(idx)\n",
    "    \n",
    "    np.save('cleanText.npy', all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "all_words = []\n",
    "all_characters = []\n",
    "all_clean_words = []\n",
    "for t in range(len(working_keys)):\n",
    "    if (t%50==0):\n",
    "        print(f\"{t//10}%\")\n",
    "    s,w,c,cw = GetStatistics(all_text[t][0])\n",
    "    all_sentences.append(s)\n",
    "    all_words.append(w)\n",
    "    all_characters.append(c)\n",
    "    all_clean_words.append(cw)\n",
    "    s,w,c,cw = GetStatistics(all_text[t][1])\n",
    "    all_sentences.append(s)\n",
    "    all_words.append(w)\n",
    "    all_characters.append(c)\n",
    "    all_clean_words.append(cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_keys = []\n",
    "for item in working_keys:\n",
    "    temp_keys.append(item)\n",
    "    temp_keys.append(item)\n",
    "\n",
    "df_diff['ID'] = temp_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff['PrePrint'] = [True if w%2==0 else False for w in range(2*len(working_keys))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for idx in range(0,2*len(working_keys),2):\n",
    "    count+=1\n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_diff = []\n",
    "for idx in range(len(working_keys)):\n",
    "    temp_diff.append(list(set(all_text[idx][0]) - set(all_text[idx][1])))\n",
    "    temp_diff.append(list(set(all_text[idx][1]) - set(all_text[idx][0])))\n",
    "\n",
    "df_diff['Difference'] = temp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_no_diff = []\n",
    "for idx in range(2*len(working_keys)):\n",
    "    temp_no_diff.append(df_diff['Difference'][idx] == [])\n",
    "\n",
    "df_diff['NoDifference'] = temp_no_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_change = []\n",
    "for idx in range(0,(2*len(working_keys))-1,2):\n",
    "    temp_change.append(df_diff['Difference'][idx] != [])\n",
    "    temp_change.append(df_diff['Difference'][idx] != [])\n",
    "    \n",
    "df_diff['TwoWayChange'] = temp_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_empty = []\n",
    "for idx in range(0,(2*len(working_keys))-1,2):\n",
    "    temp_empty.append((df_diff['NoDifference'][idx] == True) and (df_diff['NoDifference'][idx+1] == True))\n",
    "    temp_empty.append((df_diff['NoDifference'][idx] == True) and (df_diff['NoDifference'][idx+1] == True))\n",
    "    \n",
    "df_diff['BothEmpty'] = temp_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_diff.to_csv('all_diff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "lex_div = []\n",
    "for i in range(len(working_keys)):\n",
    "    #lex_div.append(lexical_diversity(all_clean_words[i]))\n",
    "    lex_div.append(lexical_diversity(all_text[i][0]))\n",
    "    lex_div.append(lexical_diversity(all_text[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lex_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\n",
    "for x in all_text[0][0]:\n",
    "    test = test+\" \"+x\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_df_all = False\n",
    "\n",
    "try:\n",
    "    df_all = pd.read_csv('all_stats.csv',index_col=[0])\n",
    "except:\n",
    "    df_all = pd.DataFrame()\n",
    "    df_all['ID'] = ['-' for u in range(2*len(working_keys))]\n",
    "    df_all['ARI'] = [-2000 for u in range(2*len(working_keys))]\n",
    "    df_all['FORCAST'] = [-2000 for u in range(2*len(working_keys))]\n",
    "    df_all['PrePrint'] = [True if w%2==0 else False for w in range(2*len(working_keys))]\n",
    "    df_all['Sentences'] = all_sentences\n",
    "    df_all['Words'] = all_words\n",
    "    df_all['Characters'] = all_characters\n",
    "    \n",
    "    create_df_all = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if create_df_all:\n",
    "    temp_keys = []\n",
    "    for item in working_keys:\n",
    "        temp_keys.append(item)\n",
    "        temp_keys.append(item)\n",
    "\n",
    "    df_all['ID'] = df_all.ID.astype(str)\n",
    "    df_all['ID'] = temp_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_df_all:\n",
    "    temp_ARI = []\n",
    "    for item in range(2*len(working_keys)):\n",
    "        temp_ARI.append(GetARI(all_sentences[item], all_words[item], all_characters[item]))\n",
    "\n",
    "    print(len(temp_ARI))\n",
    "    df_all['ARI'] = temp_ARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_df_all:\n",
    "    temp_FORCAST = []\n",
    "    for item in range(2*len(working_keys)):\n",
    "        temp_FORCAST.append(GetFORCAST(all_clean_words[item]))\n",
    "\n",
    "    print(len(temp_FORCAST))\n",
    "    df_all['FORCAST'] = temp_FORCAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_df_all:\n",
    "    temp_FRES = []\n",
    "    for item in range(2*len(working_keys)):\n",
    "        temp_FRES.append(GetFRES(all_sentences[item], all_words[item], all_clean_words[item]))\n",
    "\n",
    "    print(len(temp_FRES))\n",
    "    df_all['FRES'] = temp_FRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_df_all:\n",
    "    temp_timings = []\n",
    "    for item in all_timings.values():\n",
    "        temp_timings.append(item)\n",
    "        temp_timings.append(item)\n",
    "\n",
    "    df_all['Duration'] = temp_timings\n",
    "    \n",
    "    df_all.to_csv('all_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['ID'].isin(set(df_diff[df_diff['BothEmpty'] == False]['ID']))].describe()[:4].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[df_all['ID'].isin(set(df_diff[df_diff['BothEmpty'] == False]['ID']))].describe()[4:].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability Score Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'ARI'\n",
    "\n",
    "diff_results0 = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        diff_results0.append(df_all[col][i+1]-df_all[col][i])\n",
    "\n",
    "col = 'FORCAST'\n",
    "\n",
    "diff_results1 = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        diff_results1.append(df_all[col][i+1]-df_all[col][i])\n",
    "    \n",
    "col = 'FRES'\n",
    "\n",
    "diff_results2 = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        diff_results2.append(df_all[col][i+1]-df_all[col][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.boxplot([diff_results0,diff_results1,diff_results2], vert=False)\n",
    "ax1.grid(axis='y', alpha=0.5)\n",
    "plt.xlabel('Readability Score change (post-publication minus preprint)')\n",
    "plt.ylabel('Method (1=ARI, 2=FORCAST, 3=FRES)')\n",
    "ax1.grid(axis='x', alpha=0.5)\n",
    "plt.title('Readability score change after peer review');\n",
    "plt.savefig(\"H1_1.png\", dpi=fig.dpi*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_test_all(lst):\n",
    "    for i in lst:\n",
    "        results = sign_test(i, np.median(i))\n",
    "        print(f\"M: {results[0]} | p-value: {results[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro test for normality\n",
    "### All results show a p-value lower than 0.05, which means we can confidently reject the null-hypotheses that the data shown in the boxplot is from a normal distribution.\n",
    "##### Because of that we will have to use a non-parametric t-test to figure out if any of the readability score population means are significantly below 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapiro(diff_results0))\n",
    "print(shapiro(diff_results1))\n",
    "print(shapiro(diff_results2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric t-test called sign_test used\n",
    "#### All p-values bigger 0.05, meaning we cannot reject the null hypothesis that the average readability score change would not be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test_all([diff_results0,diff_results1,diff_results2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Diversity Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_div_results = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        lex_div_results.append(lex_div[i+1]-lex_div[i])\n",
    "        \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.boxplot([lex_div_results], vert=False)\n",
    "ax1.grid(axis='y', alpha=0.5)\n",
    "plt.xlabel('Lexical Diversity change (post-publication minus preprint)')\n",
    "plt.ylabel('Method (Lexical Diversity)')\n",
    "ax1.grid(axis='x', alpha=0.5)\n",
    "plt.title('Lexical Diversity change after peer review');\n",
    "plt.savefig(\"H1_2.png\", dpi=fig.dpi*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapiro(lex_div_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test_all([lex_div_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# < 5 Months  vs. 5+ Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Characters'\n",
    "\n",
    "less_results = []\n",
    "more_results = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        if (df_all['Duration'][i] < 2628000):\n",
    "            less_results.append(df_all['Characters'][i+1]-df_all[col][i])\n",
    "        else:\n",
    "            more_results.append(df_all['Characters'][i+1]-df_all[col][i])\n",
    "print((len(less_results),len(more_results)))\n",
    "\n",
    "print(shapiro(less_results))\n",
    "print(shapiro(more_results))\n",
    "print(mannwhitneyu(x=less_results,y=more_results, alternative = 'two-sided'))\n",
    "            \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.boxplot([less_results, more_results], vert=False)\n",
    "ax1.grid(axis='y', alpha=0.5)\n",
    "plt.xlabel(f'Amount of {col[:-1]} change after peer review')\n",
    "plt.ylabel('Group (1= less than 5 months, 2= at least 5 months)')\n",
    "plt.title(f'Amount of {col[:-1]} change');\n",
    "plt.savefig('H2_1_1.png', dpi=fig.dpi*2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Words'\n",
    "\n",
    "less_results = []\n",
    "more_results = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        if (df_all['Duration'][i] < 2628000):\n",
    "            less_results.append(df_all['Characters'][i+1]-df_all[col][i])\n",
    "        else:\n",
    "            more_results.append(df_all['Characters'][i+1]-df_all[col][i])\n",
    "print((len(less_results),len(more_results)))\n",
    "\n",
    "print(shapiro(less_results))\n",
    "print(shapiro(more_results))\n",
    "print(mannwhitneyu(x=less_results,y=more_results, alternative = 'two-sided'))\n",
    "            \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.boxplot([less_results, more_results], vert=False)\n",
    "ax1.grid(axis='y', alpha=0.5)\n",
    "plt.xlabel(f'Amount of {col[:-1]} change after peer review')\n",
    "plt.ylabel('Group (1= less than 5 months, 2= at least 5 months)')\n",
    "plt.title(f'Amount of {col[:-1]} change');\n",
    "plt.savefig('H2_1_2.png', dpi=fig.dpi*2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Sentences'\n",
    "\n",
    "less_results = []\n",
    "more_results = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        if (df_all['Duration'][i] < 2592000):\n",
    "            less_results.append(df_all['Characters'][i+1]-df_all[col][i])\n",
    "        else:\n",
    "            more_results.append(df_all['Characters'][i+1]-df_all[col][i])\n",
    "print((len(less_results),len(more_results)))\n",
    "\n",
    "print(shapiro(less_results))\n",
    "print(shapiro(more_results))\n",
    "print(mannwhitneyu(x=less_results,y=more_results, alternative = 'two-sided'))\n",
    "            \n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.boxplot([less_results, more_results], vert=False)\n",
    "ax1.grid(axis='x', alpha=0.5)\n",
    "plt.xlabel(f'Amount of {col[:-1]} change after peer review')\n",
    "plt.ylabel('Group (1= less than 5 months, 2= at least 5 months)')\n",
    "plt.title(f'Amount of {col[:-1]} change');\n",
    "plt.savefig('H2_1_3.png', dpi=fig.dpi*2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_need_duration_and_diff = pd.concat([df_all[[\"ID\",\"Duration\"]], df_diff[df_diff['BothEmpty'] == False]], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained universal sentence encoder model\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_results = []\n",
    "use_less_5_months = []\n",
    "for i in range(0,2*len(working_keys),2):\n",
    "    if (i%50==0):\n",
    "            print(f\"{100*i//(2*len(working_keys))}%\")\n",
    "    if (df_diff['BothEmpty'][i] == False):\n",
    "        use_results.append(CalculateDistanceUSE(df_diff['Difference'][i+1],df_diff['Difference'][i]))\n",
    "        use_less_5_months.append(df_need_duration_and_diff['Duration'][i])\n",
    "print(len(use_results))\n",
    "print(len(use_less_5_months))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(use_less_5_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TakeRandomIdx(chosen, quantity):\n",
    "    random.seed(777)\n",
    "    random_use = [i for i in range(len(use_results))]\n",
    "    random_use = random_use[:chosen]+random_use[chosen+1:]\n",
    "    random.shuffle(random_use)\n",
    "    output = []\n",
    "    output.append(np.abs(use_results[chosen]))\n",
    "    for f in range(quantity):\n",
    "        output.append(np.abs(CalculateDistanceUSE(df_diff['Difference'][(random_use.pop()*2)+1],df_diff['Difference'][chosen])))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(use_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.isfile(\"all_use_comparisons.npy\")):\n",
    "    # Load\n",
    "    all_use_comparisons = np.load('all_use_comparisons.npy',allow_pickle='TRUE')\n",
    "    all_use_comparisons = [list(x) for x in all_use_comparisons]\n",
    "else:\n",
    "    all_use_comparisons = []\n",
    "    for val in range(len(use_results)):\n",
    "        print(f\"{100*val//(len(use_results))}%\")\n",
    "        all_use_comparisons.append(TakeRandomIdx(val,100))\n",
    "    \n",
    "    np.save('all_use_comparisons.npy', all_use_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_use_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    \"\"\"\n",
    "    input: nasted_list - this contain any number of nested lists.\n",
    "    ------------------------\n",
    "    output: list_of_lists - one list contain all the items.\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_lists = []\n",
    "    for item in nested_list:\n",
    "        list_of_lists.extend(item)\n",
    "    return list_of_lists\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "pd.DataFrame([np.abs(x) for x in flatten(all_use_comparisons)]).plot(kind='density', ax=ax1, c=\"blue\")\n",
    "pd.DataFrame([np.abs(x) for x in use_results]).plot(kind='density', ax=ax1, c=\"darkorange\")\n",
    "plt.xlabel(\"Distance between texts (Universal Sentence Encoding)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend([\"Random paper vs post-publication comparison\",\"Preprint vs post-publication comparison\"]);\n",
    "plt.title(f'Distance in terms of similarity (Random vs paper version difference)');\n",
    "plt.savefig('H2_2_1.png', dpi=fig.dpi*2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapiro([np.abs(x) for x in flatten(all_use_comparisons)]))\n",
    "print(shapiro([np.abs(x) for x in use_results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mannwhitneyu(x=[np.abs(x) for x in use_results],y=[np.abs(x) for x in flatten(all_use_comparisons)], alternative = 'less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_lst_use = []\n",
    "right_lst_use = []\n",
    "for w in range(len(use_results)):\n",
    "    if (use_less_5_months[w] < 2592000):\n",
    "        left_lst_use.append(np.abs(use_results[w]))\n",
    "    else:\n",
    "        right_lst_use.append(np.abs(use_results[w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.boxplot([left_lst_use,right_lst_use], vert=False)\n",
    "plt.xlabel(\"Distance between texts (Universal Sentence Encoding)\")\n",
    "plt.ylabel(\"Peer-review duration (1 = took < 5 months | 2 = took 5+ months)\");\n",
    "plt.title(f'Distance in terms of similarity (between categories)');\n",
    "plt.savefig('H2_2_2.png', dpi=fig.dpi*2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapiro(left_lst_use))\n",
    "print(shapiro(right_lst_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mannwhitneyu(x=right_lst_use,y=less_results, alternative = 'greater'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
